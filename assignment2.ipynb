{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n",
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load('word2vec-google-news-300') # https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/?ref=header_search\n",
    "\n",
    "model2 = api.load('glove-wiki-gigaword-300') # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model3 = api.load('glove-wiki-gigaword-200') # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "model4 = api.load('glove-twitter-100')\n",
    "model5 = api.load('glove-wiki-gigaword-100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Debugged by ChatGPT (OpenAI, 2023; ChatGPT (Model 4.0)) https://chat.openai.com/chat\"\n",
    "def evaluate_synonym(model, model_name):\n",
    "    # Read the synonym dataset as a dataframe\n",
    "    data = pd.read_csv('synonym.csv')\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    result = []\n",
    "\n",
    "    # Iterate over each row in the dataset\n",
    "    for index, row in data.iterrows():\n",
    "        # Extract the question word and the correct answer from the current row\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "\n",
    "        # Extract the four option words from the current row\n",
    "        options = [row.iloc[2], row.iloc[3], row.iloc[4], row.iloc[5]]\n",
    "\n",
    "        # Initialize label and cosine similarity\n",
    "        label = None\n",
    "        cosine = 0\n",
    "\n",
    "        # Determine if the model is a Word2Vec model or KeyedVectors object\n",
    "        if hasattr(model, 'wv'):\n",
    "            vocabularies = model.wv.key_to_index  # For Word2Vec model\n",
    "            similarity = model.wv.similarity      # Accessing similarity method\n",
    "        else:\n",
    "            vocabularies = model.key_to_index     # For KeyedVectors object\n",
    "            similarity = model.similarity         # Accessing similarity method\n",
    "\n",
    "        # Check if the question word is in the model's vocabulary\n",
    "        if question not in vocabularies:\n",
    "            label = 'guess'\n",
    "        # Check if any of the option words are in the model's vocabulary\n",
    "        elif not any(option in vocabularies for option in options):\n",
    "            label = 'guess'\n",
    "\n",
    "        # If the label is 'guess', append the result with 'NULL' as the guess word\n",
    "        if label == 'guess':\n",
    "            result.append([question+',', answer+',', 'NULL', label])\n",
    "        else:\n",
    "            # Otherwise, evaluate each option word\n",
    "            for option in options:\n",
    "                if option in vocabularies:\n",
    "                    # Compute the cosine similarity score between the question word and the option word\n",
    "                    score = similarity(question, option)\n",
    "                    if score > cosine:\n",
    "                        # Update the best guess based on the highest similarity score\n",
    "                        temp = [question+',', answer+',', option+',', label]\n",
    "                        cosine = score\n",
    "            \n",
    "            # Determine if the best guess is correct or wrong\n",
    "            if temp[1] == temp[2]:\n",
    "                temp[3] = 'correct'\n",
    "            else:\n",
    "                temp[3] = 'wrong'\n",
    "\n",
    "            # Append the result\n",
    "            result.append(temp)\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    output = pd.DataFrame(result, columns=['question', 'answer', 'guess', 'label'])\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    output.to_csv(model_name+'-details.csv', index=False)\n",
    "\n",
    "    # Calculate the number of correct labels and the number of questions answered without guessing\n",
    "    correct_labels = output['label'].value_counts().get('correct', 0)\n",
    "    questions_answered = output['label'].value_counts().sum() - output['label'].value_counts().get('guess', 0)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = correct_labels / questions_answered if questions_answered > 0 else 0\n",
    "\n",
    "    # Print and return the results\n",
    "    print([model_name, str(len(vocabularies))+',', str(correct_labels)+',', str(questions_answered)+',', accuracy])\n",
    "    return [model_name, str(len(vocabularies))+',', str(correct_labels)+',', str(questions_answered)+',', accuracy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec-google-news-300', '3000000,', '70,', '79,', 0.8860759493670886]\n"
     ]
    }
   ],
   "source": [
    "list1 = evaluate_synonym(model, 'word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-wiki-gigaword-300', '400000,', '71,', '80,', 0.8875]\n",
      "['glove-wiki-gigaword-200', '400000,', '68,', '80,', 0.85]\n",
      "['glove-twitter-100', '1193514,', '39,', '78,', 0.5]\n",
      "['glove-wiki-gigaword-100', '400000,', '65,', '80,', 0.8125]\n"
     ]
    }
   ],
   "source": [
    "list2 = evaluate_synonym(model2, 'glove-wiki-gigaword-300')\n",
    "list3 = evaluate_synonym(model3, 'glove-wiki-gigaword-200')\n",
    "list4 = evaluate_synonym(model4, 'glove-twitter-100')\n",
    "list5 = evaluate_synonym(model5, 'glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(list, list2, list3, list4, list5):\n",
    "    analysis = pd.DataFrame([list, list2, list3, list4,list5], columns = ['model', 'vocabulary_size', 'correct_labels', 'questions_answered', 'accuracy'])\n",
    "    analysis.to_csv('analysis.csv', index = False)\n",
    "analyze(list1, list2, list3, list4, list5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train your Own Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessed_text(book):\n",
    "    with open(book, 'r', encoding='utf-8') as file:\n",
    "        # Read the entire content of the file into a single string\n",
    "        text = file.read()\n",
    "\n",
    "    # Use NLTK to split the text into a list of sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    processed_sentence = []\n",
    "\n",
    "    # Iterate over each sentence in the list of sentences\n",
    "    for sentence in sentences:\n",
    "        # Convert the sentence to lowercase and tokenize it into words\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "        # Filter out tokens that are not purely alphabetic\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "        # Append the processed tokens (words) to the processed_sentence list\n",
    "        processed_sentence.append(tokens)\n",
    "\n",
    "    # Return the list of processed sentences\n",
    "    return processed_sentence\n",
    "\n",
    "type(preprocessed_text('book1.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text:list, window:int, embedding:int):\n",
    "    model_name = f\"Word2Vec_e{embedding}_w{window}\"\n",
    "    model = gensim.models.Word2Vec(sentences=text, window=window, vector_size=embedding)\n",
    "    return model_name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessed_text('five_books.txt')\n",
    "\n",
    "my_model_name_1, my_model_1= train(text, 5, 300) # window 5 embedding 300\n",
    "my_model_name_2, my_model_2= train(text, 10, 300) # window 10 embedding 300\n",
    "\n",
    "my_model_name_3, my_model_3= train(text, 5, 200) # window 5 embedding 200\n",
    "my_model_name_4, my_model_4= train(text, 10, 200) # wiondow 10 embedding 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Word2Vec_e300_w5', '3026,', '5,', '14,', 0.35714285714285715]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Word2Vec_e300_w5', '3026,', '5,', '14,', 0.35714285714285715]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = evaluate_synonym(my_model_1, my_model_name_1)\n",
    "result2 = evaluate_synonym(my_model_2, my_model_name_2)\n",
    "result3 = evaluate_synonym(my_model_3, my_model_name_3)\n",
    "result4 = evaluate_synonym(my_model_4, my_model_name_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframe(result1, result2, result3, result4):\n",
    "    current_df = pd.read_csv('analysis.csv')\n",
    "    new_df = pd.DataFrame([result1, result2, result3, result4], columns=current_df.columns)\n",
    "    merged_df = pd.concat([current_df, new_df])\n",
    "    merged_df.to_csv('analysis.csv', index = False)\n",
    "    return merged_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataframe(result1, result2, result3, result4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
