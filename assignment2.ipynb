{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Task 1: Evaluation of the word2vec-google-news-300 Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n",
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model = api.load('word2vec-google-news-300') # https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/?ref=header_search\n",
    "\n",
    "model2 = api.load('glove-wiki-gigaword-300') # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model3 = api.load('glove-wiki-gigaword-200') # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "model4 = api.load('glove-twitter-100')\n",
    "model5 = api.load('glove-wiki-gigaword-100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synonym(model, model_name):\n",
    "    data = pd.read_csv('synonym.csv')\n",
    "    result = []\n",
    "    for index, row in data.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        options = [row.iloc[2], row.iloc[3], row.iloc[4], row.iloc[5]]\n",
    "        label = None\n",
    "        cosine = 0\n",
    "\n",
    "        if hasattr(model, 'wv'):\n",
    "            vocabularies = model.wv.key_to_index\n",
    "            similarity = model.wv.similarity\n",
    "        else:\n",
    "            vocabularies = model.key_to_index\n",
    "            similarity = model.similarity\n",
    "\n",
    "        if question not in vocabularies:\n",
    "            label = 'guess'\n",
    "        if not any(option in vocabularies for option in options):\n",
    "            label = 'guess'\n",
    "        if label == 'guess':\n",
    "            result.append([question+',', answer+',', 'NULL', label])\n",
    "        else:\n",
    "            for option in options:\n",
    "                if option in vocabularies:\n",
    "                    score = similarity(question, option)\n",
    "                    if score > cosine:\n",
    "                        temp = [question+',', answer+',', option+',', label]\n",
    "                        cosine = score\n",
    "            if temp[1] == temp[2]:\n",
    "                temp[3] = 'correct'\n",
    "            else:\n",
    "                temp[3] = 'wrong'\n",
    "            result.append(temp)\n",
    "    output = pd.DataFrame(result, columns=['question', 'answer', 'guess', 'label'])\n",
    "    output.to_csv(model_name+'-details.csv', index=False)\n",
    "    correct_labels = output['label'].value_counts().get('correct', 0)\n",
    "    questions_answered = output['label'].value_counts().sum() - output['label'].value_counts().get('guess', 0)\n",
    "    accuracy = correct_labels / questions_answered\n",
    "    print([model_name, str(len(vocabularies))+',', str(correct_labels)+',', str(questions_answered)+',', accuracy])\n",
    "    return [model_name, str(len(vocabularies))+',', str(correct_labels)+',', str(questions_answered)+',', accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec-google-news-300', '3000000,', '70,', '79,', 0.8860759493670886]\n"
     ]
    }
   ],
   "source": [
    "list1 = evaluate_synonym(model, 'word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Comparison with Other Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-wiki-gigaword-300', '400000,', '71,', '80,', 0.8875]\n",
      "['glove-wiki-gigaword-200', '400000,', '68,', '80,', 0.85]\n",
      "['glove-twitter-100', '1193514,', '39,', '78,', 0.5]\n",
      "['glove-wiki-gigaword-100', '400000,', '65,', '80,', 0.8125]\n"
     ]
    }
   ],
   "source": [
    "list2 = evaluate_synonym(model2, 'glove-wiki-gigaword-300')\n",
    "list3 = evaluate_synonym(model3, 'glove-wiki-gigaword-200')\n",
    "list4 = evaluate_synonym(model4, 'glove-twitter-100')\n",
    "list5 = evaluate_synonym(model5, 'glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(list, list2, list3, list4, list5):\n",
    "    analysis = pd.DataFrame([list, list2, list3, list4,list5], columns = ['model', 'vocabulary_size', 'correct_labels', 'questions_answered', 'accuracy'])\n",
    "    analysis.to_csv('analysis.csv', index = False)\n",
    "analyze(list1, list2, list3, list4, list5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train your Own Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessed_text(book):\n",
    "    with open(book, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentence = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        processed_sentence.append(tokens)\n",
    "    return processed_sentence\n",
    "type(preprocessed_text('book1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text:list, window:int, embedding:int):\n",
    "    model_name = f\"Word2Vec_e{embedding}_w{window}\"\n",
    "    model = gensim.models.Word2Vec(sentences=text, window=window, vector_size=embedding)\n",
    "    return model_name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocessed_text('book1.txt')\n",
    "mymodel_name, my_model= train(text, 5, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Word2Vec_e300_w5', '3026,', '5,', '14,', 0.35714285714285715]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Word2Vec_e300_w5', '3026,', '5,', '14,', 0.35714285714285715]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_synonym(my_model, mymodel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
